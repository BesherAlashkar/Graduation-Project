{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1REsHdyJ2XMZ",
        "rF1kI36mQ8it",
        "d06PKED3N53i",
        "YRYtWowrcxg9",
        "AqOGDB6H3DJW",
        "tNERwZt1V_TR",
        "3hVKJpY1uaTc",
        "9P-R7G9ucBok",
        "9GA6Rqq3zMdM",
        "v16jHbrv8U-9",
        "pi904rruzhqx",
        "XHdS9jukhzcf",
        "i6KcZziKAFH2"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Read the data"
      ],
      "metadata": {
        "id": "1REsHdyJ2XMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "yHwvz69P2ay6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XjHzIbbg2ghh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing = pd.read_csv(\"/content/drive/MyDrive/Graduation Project/Data/housing.csv\")"
      ],
      "metadata": {
        "id": "dpw4NOWv2zvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing"
      ],
      "metadata": {
        "id": "vBDreHGYZ0Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Initial Data Exploration"
      ],
      "metadata": {
        "id": "rF1kI36mQ8it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing.info()"
      ],
      "metadata": {
        "id": "WwQwa6fbRBfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.shape"
      ],
      "metadata": {
        "id": "KMbLI5ZgfUjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.columns"
      ],
      "metadata": {
        "id": "aAWz_PZkXmYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing[\"ocean_proximity\"].value_counts()"
      ],
      "metadata": {
        "id": "8R9XyMW7RTzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.describe()"
      ],
      "metadata": {
        "id": "UquLJ1zOOhCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.describe(include=\"O\") #include=\"O\" specifies only object columns"
      ],
      "metadata": {
        "id": "rN8rXlBsPErb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "housing.hist(bins=50, figsize=(20,15));"
      ],
      "metadata": {
        "id": "UeJNdT8LPIf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Split the Data"
      ],
      "metadata": {
        "id": "d06PKED3N53i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2.1Train And Test sets**###"
      ],
      "metadata": {
        "id": "YRYtWowrcxg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
        "\n",
        "train_set, val_set = train_test_split(train_set, test_size=0.25, random_state=42)  # 0.25 of 0.8 = 20%\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4r11dnQ0N-cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Set Shape:\", train_set.shape)\n",
        "print(\"Validation Set Shape:\", val_set.shape)\n",
        "print(\"Test Set Shape:\", test_set.shape)"
      ],
      "metadata": {
        "id": "P9yuSfK6SGQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_eda = train_set.copy()"
      ],
      "metadata": {
        "id": "PE90ZzfLZgBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Discover and Visualize the Data to Gain Insights"
      ],
      "metadata": {
        "id": "AqOGDB6H3DJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.1 Visualizing Geographical Data**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tNERwZt1V_TR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "train_set_eda.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\");"
      ],
      "metadata": {
        "id": "xRqLcInU3HeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_eda.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1);"
      ],
      "metadata": {
        "id": "qk-S86QRJ3GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_eda.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
        "             s=train_set_eda[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
        "             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
        "             sharex=False);"
      ],
      "metadata": {
        "id": "mok7WkgqLIVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.2 Looking for Correlations**"
      ],
      "metadata": {
        "id": "IO3mtGvTYKiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_columns = train_set_eda.select_dtypes(include=['float64', 'int64'])\n",
        "correlation_matrix = numeric_columns.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap of Numeric Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UJnZdl-HZhxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "Fw3yiMy4TB92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
        "\n",
        "import seaborn as sns\n",
        "sns.pairplot(train_set_eda[attributes]);"
      ],
      "metadata": {
        "id": "zKGBctf9TF6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_eda.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1);"
      ],
      "metadata": {
        "id": "F1BZOGc9Tfc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_set_eda.columns)\n",
        "train_set_eda.head(1)"
      ],
      "metadata": {
        "id": "kTiKope6Tg5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.3 Experimenting with Attribute Combinations**"
      ],
      "metadata": {
        "id": "IFcDTdLMKPWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_eda['rooms_per_household'] = train_set_eda['total_rooms'] / train_set_eda['households']\n",
        "train_set_eda['bedrooms_per_room'] = train_set_eda['total_bedrooms'] / train_set_eda['total_rooms']\n",
        "train_set_eda['population_per_household'] = train_set_eda['population'] / train_set_eda['households']"
      ],
      "metadata": {
        "id": "NGJhuIo2KUyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = train_set_eda.select_dtypes(include=['float64', 'int64']).corr()\n",
        "correlation_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "mvz0cgIBToKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Prepare the Data for Machine Learning Algorithms"
      ],
      "metadata": {
        "id": "Sdj4sVWFhw5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.1 Data Cleaning**###"
      ],
      "metadata": {
        "id": "3hVKJpY1uaTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TotalBedroomsMedianFiller(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.median_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Calculate the median of the 'total_bedrooms' column\n",
        "        self.median_ = X[\"total_bedrooms\"].median()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Fill missing values in the 'total_bedrooms' column with the calculated median\n",
        "        X_copy = X.copy()\n",
        "        X_copy[\"total_bedrooms\"] = X_copy[\"total_bedrooms\"].fillna(self.median_)\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "gZRrt3ZHt75Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.2 One-Hot Encoding**###"
      ],
      "metadata": {
        "id": "9P-R7G9ucBok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pandas as pd\n",
        "\n",
        "class OceanProximityOneHotEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.encoded_columns = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Get the one-hot encoded column names\n",
        "        self.encoded_columns = pd.get_dummies(X[\"ocean_proximity\"], drop_first=True).columns.tolist()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Perform one-hot encoding and drop the original column\n",
        "        X_copy = X.copy()\n",
        "        one_hot_encoded = pd.get_dummies(X_copy[\"ocean_proximity\"], drop_first=True)\n",
        "        X_copy = X_copy.drop(columns=[\"ocean_proximity\"])\n",
        "        X_copy = pd.concat([X_copy, one_hot_encoded], axis=1)\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "Na63MQ_ubPZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.3Custom Transformers**###"
      ],
      "metadata": {
        "id": "9GA6Rqq3zMdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
        "\n",
        "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Ensure X is a NumPy array\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "\n",
        "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
        "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
        "        bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "        return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n"
      ],
      "metadata": {
        "id": "CSYXO3w1zO8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.4FeatureScaler**###"
      ],
      "metadata": {
        "id": "v16jHbrv8U-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class FeatureScaler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.scaler.fit(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.scaler.transform(X)"
      ],
      "metadata": {
        "id": "TasllIFL8N7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.5Transformation Pipelines**###"
      ],
      "metadata": {
        "id": "pi904rruzhqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split X and y\n",
        "X_train = train_set.drop(\"median_house_value\", axis=1)\n",
        "y_train = train_set[\"median_house_value\"]"
      ],
      "metadata": {
        "id": "Iv-8Asei9hwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "full_pipeline = Pipeline([\n",
        "    (\"total_bedrooms_filler\", TotalBedroomsMedianFiller()),\n",
        "    (\"ocean_proximity_encoder\", OceanProximityOneHotEncoder()),\n",
        "    (\"attribute_adder\", CombinedAttributesAdder()),\n",
        "    (\"feature_scaler\", FeatureScaler())\n",
        "])\n",
        "\n",
        "X_train_prepared = full_pipeline.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "Td3H5l0nzkVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split X and y\n",
        "X_val = val_set.drop(\"median_house_value\", axis=1)\n",
        "y_val = val_set[\"median_house_value\"]"
      ],
      "metadata": {
        "id": "mNW3586g-aGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_prepared = full_pipeline.transform(X_val)"
      ],
      "metadata": {
        "id": "Gcs_3EHu-Xnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e349d25e-f6fa-4e91-b501-3dd4f36551e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Select and Train a Model"
      ],
      "metadata": {
        "id": "XHdS9jukhzcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import Ridge"
      ],
      "metadata": {
        "id": "k8m-GF4ZiNUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_distributions = {\n",
        "    'random_forest': {\n",
        "        'n_estimators': [50, 100, 200, 300],\n",
        "        'max_features': ['sqrt', 'log2', None],  # Remove 'auto'\n",
        "        'max_depth': [None, 10, 20, 30, 40],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'svr': {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': ['scale', 'auto'],\n",
        "        'kernel': ['linear', 'rbf', 'poly']\n",
        "    },\n",
        "    'ridge': {\n",
        "        'alpha': [0.1, 1, 10, 100],\n",
        "        'solver': ['auto', 'svd', 'cholesky', 'lsqr']\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "YgJoqFey_RwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_distributions"
      ],
      "metadata": {
        "id": "TPkhK-w_MkJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'random_forest': RandomForestRegressor(random_state=42),\n",
        "    'svr': SVR(),\n",
        "    'ridge': Ridge()\n",
        "}\n",
        "\n",
        "# Store results\n",
        "model_results = {}\n",
        "\n",
        "# Perform Randomized Search for each model\n",
        "for model_name, model in models.items():\n",
        "    search = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=param_distributions[model_name],\n",
        "        n_iter=20,  # Number of random parameter combinations to try\n",
        "        scoring='neg_mean_squared_error',\n",
        "        cv=3,  # Cross-validation folds\n",
        "        verbose=1,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Fit the search\n",
        "    search.fit(X_train_prepared, y_train)\n",
        "\n",
        "    # Save the results for the current model\n",
        "    model_results[model_name] = {\n",
        "        'best_estimator': search.best_estimator_,\n",
        "        'best_params': search.best_params_,\n",
        "        'best_score': np.sqrt(-search.best_score_),\n",
        "        'cv_results': search.cv_results_\n",
        "    }\n",
        "\n",
        "# Print results for all models\n",
        "for model_name, results in model_results.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Best Parameters: {results['best_params']}\")\n",
        "    print(f\"Best Validation RMSE: {results['best_score']:.4f}\")\n",
        "    print(\"\\nDetailed CV Results:\")\n",
        "    for i, mean_score in enumerate(results['cv_results']['mean_test_score']):\n",
        "        print(f\"  Combination {i+1}: {results['cv_results']['params'][i]} -> RMSE: {np.sqrt(-mean_score):.4f}\")\n",
        "\n",
        "# Select the best model overall\n",
        "best_model_name = min(model_results, key=lambda name: model_results[name]['best_score'])\n",
        "best_model = model_results[best_model_name]['best_estimator']\n",
        "\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"Best Parameters: {model_results[best_model_name]['best_params']}\")\n",
        "print(f\"Best Validation RMSE: {model_results[best_model_name]['best_score']:.4f}\")\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "y_val_predictions = best_model.predict(X_val_prepared)\n",
        "final_rmse = np.sqrt(mean_squared_error(y_val, y_val_predictions))\n",
        "print(f\"\\nFinal RMSE on Validation Set: {final_rmse:.4f}\")"
      ],
      "metadata": {
        "id": "SaXon91sQ-57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "i6KcZziKAFH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split X and y for the test set\n",
        "X_test = test_set.drop(\"median_house_value\", axis=1)\n",
        "y_test = test_set[\"median_house_value\"]\n",
        "\n",
        "# Prepare the test set\n",
        "X_test_prepared = full_pipeline.transform(X_test)\n",
        "\n",
        "# Use the best model to make predictions\n",
        "test_predictions = best_model.predict(X_test_prepared)\n",
        "\n",
        "# Add predictions as a new column to the original test data\n",
        "test_set_with_predictions = test_set.copy()\n",
        "test_set_with_predictions[\"predicted_median_house_value\"] = test_predictions"
      ],
      "metadata": {
        "id": "UrdgXcBUAnhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set_with_predictions"
      ],
      "metadata": {
        "id": "RTYdaEcAAqqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to a CSV file\n",
        "output_file = \"test_set_with_predictions.csv\"\n",
        "test_set_with_predictions.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Results saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezYFS2QtAuld",
        "outputId": "909524f2-7598-4b33-ccc0-9e647eb5d2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to test_set_with_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_14uyGsGBFSu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}